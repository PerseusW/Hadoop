"best"	1@doc2;
(and	1@doc1;1@doc3;
(collection	1@doc2;
(or	1@doc3;
(since	1@doc2;
3	1@doc2;
<key,	1@doc2;
A	1@doc1;1@doc3;
All	1@doc1;
Applications	1@doc1;
CompressionCodecs	1@doc1;
Configurable	1@doc3;
Configuration	1@doc1;1@doc2;
Configuration.	1@doc1;
Finally	1@doc1;
For	1@doc2;
HTTP	1@doc2;
Hadoop	1@doc1;
Hence	1@doc3;
If	1@doc1;1@doc3;
In	1@doc2;
Input	2@doc2;
InputFormat	1@doc1;
InputSplit	1@doc1;
InputSplit.	1@doc1;
Iterable,	1@doc2;
Job's	1@doc3;
Job.setCombinerClass(Class),	1@doc1;
Job.setGroupingComparatorClass(Class).	1@doc2;
Job.setSortComparatorClass(Class).	1@doc2;
JobContext.getConfiguration()	1@doc2;
JobContext.getConfiguration().	1@doc1;
Key:	2@doc2;
Map	4@doc2;
Map-Reduce	1@doc1;
Mapper	4@doc1;1@doc2;
Mappers	1@doc2;
Maps	1@doc1;
Note:	2@doc3;
Object).	1@doc2;
Object,	1@doc1;
Output	2@doc2;
OutputFormat	1@doc1;
OutputKeyComparator:	1@doc2;
OutputValueGroupingComparator:	1@doc2;
Partitioner	3@doc3;
Partitioner.	1@doc1;
Partitioner:	1@doc2;
Partitions	1@doc3;
RawComparator	1@doc1;
RecordWriter	1@doc2;
Reduce	1@doc2;
Reducer	5@doc2;2@doc1;
Reducer.	2@doc1;
Reduces	1@doc2;
SecondarySort	1@doc2;
Shuffle	1@doc2;
Sort	1@doc2;
TaskInputOutputContext.write(Object,	1@doc2;
The	7@doc2;4@doc1;2@doc3;
To	1@doc2;
Users	3@doc1;
Value:	2@doc2;
You	1@doc2;
a	6@doc1;6@doc2;2@doc3;
access	1@doc1;1@doc2;
achieve	1@doc2;
across	1@doc2;
aggregation	1@doc1;
all	1@doc2;
amount	1@doc1;
and	5@doc2;4@doc1;
application	1@doc2;
aps	1@doc1;
are	5@doc1;3@doc2;1@doc3;
as	1@doc1;1@doc3;
associated	1@doc1;
be	3@doc1;2@doc2;
being	1@doc2;
but	1@doc2;
by	6@doc1;6@doc2;1@doc3;
call	1@doc2;
called	1@doc2;
called.	1@doc1;
calls	1@doc1;
can	5@doc1;1@doc2;
checksum	3@doc2;
checksum,	1@doc2;
class	1@doc3;
classes.	1@doc1;
cleanup(org.apache.hadoop.mapreduce.Mapper.Context)	1@doc1;
combiner,	1@doc1;
comparator	2@doc2;
comparator.	1@doc2;
compressed	1@doc1;
configuration	1@doc3;
control	2@doc1;
controlled	1@doc2;
controls	2@doc3;
copies	1@doc2;
created	1@doc3;
custom	1@doc1;
cut	1@doc1;
data	1@doc1;
decide	1@doc2;
decreasing	1@doc2;
define	1@doc2;
derive	1@doc3;
determine	1@doc1;
different	1@doc2;
directly	1@doc1;
document	2@doc2;
down	1@doc1;
duplicate	1@doc2;
each	2@doc1;2@doc2;
entire	1@doc2;
example,	1@doc2;
example.	1@doc2;
extend	1@doc2;
fetched	1@doc2;
final	1@doc1;
find	1@doc2;
first	1@doc1;
followed	1@doc1;
for	4@doc1;2@doc2;2@doc3;
framework	2@doc1;1@doc2;
framework,	1@doc1;
from	1@doc1;1@doc2;
function.	1@doc3;
generated	1@doc1;
given	2@doc1;
go	1@doc1;
grouped	1@doc1;1@doc2;
grouping	3@doc2;1@doc1;
has	1@doc1;1@doc2;
hash	1@doc3;
have	1@doc2;
helps	1@doc1;
hence	1@doc1;1@doc3;
how	1@doc1;
i.e.	1@doc2;
if	1@doc1;
implement	1@doc3;
implementations	1@doc1;1@doc2;
implementing	1@doc1;
in	2@doc2;1@doc1;
individual	1@doc1;
input	4@doc1;
inputs	1@doc2;
inputs.	1@doc2;
interface.	1@doc3;
intermediate	6@doc1;2@doc3;1@doc2;
into	1@doc1;
is	5@doc2;4@doc3;2@doc1;
iterator,	1@doc2;
job	2@doc1;2@doc2;
job.	1@doc1;1@doc3;
key	3@doc2;3@doc3;2@doc1;
key)	1@doc3;
key).	1@doc2;
key,	1@doc2;
key/value	3@doc1;
keys	3@doc2;1@doc1;1@doc3;
keys.	1@doc1;
known	1@doc2;
like:	1@doc2;
local	1@doc1;
m	1@doc3;
many	1@doc1;
map	2@doc1;
map(Object,	1@doc1;
map-outputs.	1@doc3;
may	1@doc1;1@doc2;
merge	1@doc2;
merged.	1@doc2;
method	1@doc2;
method.	1@doc2;
multiple	1@doc3;
need	1@doc1;
network.	1@doc2;
not	1@doc1;1@doc2;
number	2@doc3;
object,	1@doc3;
obtain	1@doc3;
occur	1@doc2;
of	6@doc2;6@doc3;5@doc1;
on	1@doc2;
one	1@doc1;
only	1@doc3;
optionally	1@doc1;
or	1@doc1;
order	1@doc2;
org.apache.hadoop.mapreduce.Mapper.Context)	1@doc1;
org.apache.hadoop.mapreduce.Reducer.Context)	1@doc2;
output	4@doc2;3@doc1;
output.	1@doc1;
outputs	2@doc1;1@doc2;
outputs,	1@doc1;
pagerank	2@doc2;
pages	1@doc2;
pair	2@doc1;
pairs	1@doc1;
pairs.	2@doc1;
partition,	1@doc3;
partitioned	1@doc1;
partitioning	1@doc3;
partitions	1@doc3;
passed	1@doc1;
per	1@doc1;
perform	1@doc1;
phase	1@doc2;
phases	1@doc2;
phases:	1@doc2;
primary	1@doc2;
re-sorted.	1@doc2;
record)	1@doc3;
records	2@doc1;
records)	1@doc1;
records.	2@doc1;
reduce	2@doc3;1@doc2;
reduce(Object,	1@doc2;
reduce.The	1@doc2;
reducers.	1@doc3;
reduces	1@doc1;
reduction.	1@doc3;
require	1@doc3;
returned	1@doc2;
same	2@doc2;1@doc1;1@doc3;
say	1@doc2;
secondary	2@doc2;
sent	1@doc2;1@doc3;
set	3@doc2;1@doc1;
setup(org.apache.hadoop.mapreduce.Mapper.Context),	1@doc1;
share	1@doc2;
should	1@doc2;
shuffle	1@doc2;
simultaneously	1@doc2;
smaller	1@doc2;
sort	3@doc2;
sorted	3@doc2;
sorting	2@doc1;
sorts	1@doc2;
space.	1@doc3;
spawns	1@doc1;
specified	1@doc2;
specify	2@doc1;
specifying	1@doc1;
subsequently	1@doc1;
subset	1@doc3;
tag	1@doc2;
task	1@doc1;1@doc2;
tasks	2@doc3;1@doc1;
that	1@doc2;
the	22@doc1;21@doc2;14@doc3;
them	1@doc2;
then	1@doc1;1@doc2;
there	1@doc3;
they	1@doc2;
this	1@doc2;1@doc3;
to	11@doc1;5@doc2;2@doc3;
total	1@doc3;
transferred	1@doc1;
transform	1@doc1;
transformed	1@doc1;
two	1@doc1;
type	1@doc1;
typically	1@doc2;1@doc3;
up	1@doc2;
url	4@doc2;
used	1@doc1;1@doc3;
using	3@doc2;
value	1@doc2;
values	3@doc2;1@doc1;
values)>	1@doc2;
values.	1@doc2;
via	3@doc1;3@doc2;
want	1@doc2;
web	1@doc2;
when	1@doc3;
which	5@doc1;2@doc2;1@doc3;
while	1@doc2;
will	2@doc2;
with	2@doc2;1@doc1;
without	1@doc1;
would	1@doc2;
written	1@doc1;1@doc2;
you	1@doc2;1@doc3;
your	1@doc3;
zero	2@doc1;
